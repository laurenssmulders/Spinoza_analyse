{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse van overeenkomsten tussen verschillende vertalingen van Spinoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorbewerking van de teksten\n",
    "\n",
    "### Strippen en splitsen in zinnen\n",
    "Allereerst moeten bij de voorbewerking al aanwezige enters, witregels, etc. verwijderd worden. Op die manier ontstaat een doorlopende tekst, die makkelijker te interpreteren is voor de computer. Hiervoor heb ik de functie 'superstrip()' geschreven.\n",
    "Vervolgens is aan de hand van interpunctie de tekst opgesplitst in losse zinnen, gescheiden door een enter. Hiervoor heb ik eerst zelf een functie geschreven ('sentence_split()'); later heb ik een functie geimporteerd van de nltk ('nltk.tokenize.sent_tokenize()'). Ik heb deze functies de tekst uit een document laten nemen om de bewerkte tekst vervolgens in een nieuw document op te slaan. De precieze functies staan hieronder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superstrip(path_bron, path_doel):\n",
    "    import codecs\n",
    "    file = codecs.open(path_bron, 'r', 'utf-8')\n",
    "    lines = [line.strip() for line in file]\n",
    "    text = \" \".join(lines)\n",
    "    file.close()\n",
    "    file = codecs.open(path_doel, 'w', 'utf-8')\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "\n",
    "def sentence_split(Path_bron, Path_doel):\n",
    "    superstrip(Path_bron, Path_doel)\n",
    "    import codecs\n",
    "    file = codecs.open(Path_doel, 'r', 'utf-8') \n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    zinnen = text.split('. ')\n",
    "    \n",
    "    text = '.\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('! ')\n",
    "    \n",
    "    text = '!\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('? ')\n",
    "    \n",
    "    text = '?\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('\\n')\n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 0:\n",
    "            if zin[0].islower():\n",
    "                zinnen[zinnen.index(zin)-1] += ' ' + zin\n",
    "                zinnen.remove(zin)\n",
    "            elif zin[0] == ')':\n",
    "                zinnen[zinnen.index(zin)-1] += ' ' + zin\n",
    "                zinnen.remove(zin)\n",
    "            \n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 3:\n",
    "            if zin[-3:] == \"Mr.\" or zin[-3:] == \"Dr.\" or zin[-4:] == \"Mrs.\" or zin[-3:] == \"Ms.\":\n",
    "                zinnen[zinnen.index(zin)+1] = zin + ' ' +  zinnen[zinnen.index(zin)+1]\n",
    "                zinnen.remove(zin)\n",
    "    \n",
    "    text = '\\n'.join(zinnen)\n",
    "    \n",
    "    \n",
    "    file = codecs.open(Path_doel, 'w', 'utf-8') \n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "    superstrip(Path_doel, Path_doel)\n",
    "    \n",
    "    import codecs\n",
    "    file = codecs.open(Path_doel, 'r', 'utf-8') \n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    zinnen = text.split('. ')\n",
    "    \n",
    "    text = '.\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('! ')\n",
    "    \n",
    "    text = '!\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('? ')\n",
    "    \n",
    "    text = '?\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('\\n')\n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 0:\n",
    "            if zin[0].islower():\n",
    "                zinnen[zinnen.index(zin)-1] += ' ' + zin\n",
    "                zinnen.remove(zin)\n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 3:\n",
    "            if zin[-3:] == \"Mr.\" or zin[-3:] == \"Dr.\" or zin[-4:] == \"Mrs.\" or zin[-3:] == \"Ms.\":\n",
    "                zinnen[zinnen.index(zin)+1] = zin + ' ' +  zinnen[zinnen.index(zin)+1]\n",
    "                zinnen.remove(zin)\n",
    "    \n",
    "    text = '\\n'.join(zinnen)\n",
    "    \n",
    "    \n",
    "    file = codecs.open(Path_doel, 'w', 'utf-8') \n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "    superstrip(Path_doel, Path_doel)\n",
    "    \n",
    "    import codecs\n",
    "    file = codecs.open(Path_doel, 'r', 'utf-8') \n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    zinnen = text.split('. ')\n",
    "    \n",
    "    text = '.\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('! ')\n",
    "    \n",
    "    text = '!\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('? ')\n",
    "    \n",
    "    text = '?\\n'.join(zinnen)\n",
    "    \n",
    "    zinnen = text.split('\\n')\n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 0:\n",
    "            if zin[0].islower():\n",
    "                zinnen[zinnen.index(zin)-1] += ' ' + zin\n",
    "                zinnen.remove(zin)\n",
    "    \n",
    "    for zin in zinnen:\n",
    "        if len(zin) > 3:\n",
    "            if zin[-3:] == \"Mr.\" or zin[-3:] == \"Dr.\" or zin[-4:] == \"Mrs.\" or zin[-3:] == \"Ms.\":\n",
    "                zinnen[zinnen.index(zin)+1] = zin + ' ' +  zinnen[zinnen.index(zin)+1]\n",
    "                zinnen.remove(zin)\n",
    "    \n",
    "    text = '\\n'.join(zinnen)\n",
    "    \n",
    "    \n",
    "    file = codecs.open(Path_doel, 'w', 'utf-8') \n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "def nltktokenize(path_bron, path_doel):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    import codecs\n",
    "    file = codecs.open(path_bron, 'r', 'utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    text = sent_tokenize(text, language='dutch')\n",
    "    text = '\\n'.join(text)\n",
    "    file = codecs.open(path_doel, 'w', 'utf-8')\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Overbodige tekens en hoofdletters verwijderen\n",
    "Bij het interpreteren van de tekst gaat het alleen om de woorden. Zodra de teksten dus gesplitst zijn in zinnen, moet alle interpunctie verwijderd worden. De functie 'string.interpunction()' geeft een lijst van veelgebruikte leestekens. Daarnaast heb ik zelf nog een paar leestekens die in deze teksten voorkwamen toegevoegd.\n",
    "\n",
    "Om de teksten goed te vergelijken moet het algoritme niet hoofdlettergevoelig zijn. Hierom heb ik alle hoofdletters in de tekst geconverteerd naar kleine letters.\n",
    "\n",
    "In de 17e eeuw was er nog geen uniforme Nederlandse spelling. Anders gespelde woorden moet de computer echter wel als hetzelfde herkennen. Bij bestudering van de teksten valt op dat de verschillen in spelling vaak verschillen in het gebruik van klinkers zijn. Ook het gebruik van de letters d, t, s, z en j verschilt erg. Deze letters gecombineerd met de klinkers zijn dus blijkbaar niet kenmerkend voor een woord. Ik heb het algoritme daarom al deze letters uit de tekst laten verwijderen.\n",
    "\n",
    "De functie die ik gebruikt heb om deze drie taken uit te voeren heb ik 'interpunction_uppercase_remove()' genoemd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpunction_uppercase_remove_text(text):\n",
    "    import string\n",
    "    kenmerkende_letters = ['q', 'w', 'r', 'p', 'f', 'g', 'h', 'k', 'l', 'x', 'c', 'v', 'b', 'n', 'm']\n",
    "    niet_kenmerkende_letters = ['e', 't', 'y', 'u', 'i', 'o', 'a', 'd', 'j', 'z']\n",
    "    interpunctie = list(string.punctuation)\n",
    "    interpunctie.append(\"’\")\n",
    "    interpunctie.append(\"‘\")\n",
    "    text = text.lower()\n",
    "    for letter in niet_kenmerkende_letters :\n",
    "        text = text.replace(letter, '')\n",
    "    for letter in interpunctie:\n",
    "        text = text.replace(letter, '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Chunktokenizer\n",
    "Doordat in de ene tekst meer zinnen samengevoegd zijn dan in de andere tekst, levert het splitsen in zinnen in de ene tekst langere blokken op dan in de andere tekst. Ter vervanging van deze manier heb ik dus nog een functie geschreven: 'chunktokenizer()'. Deze functie splitst de tekst op in duizend blokken met evenveel woorden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunktokenizer(path_bron, path_doel):\n",
    "    import codecs\n",
    "    file = codecs.open(path_bron, 'r', 'utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    words = text.split()\n",
    "    number_of_words = len(words)\n",
    "    words_per_chunk = int(round(number_of_words/1000))\n",
    "    start = 0\n",
    "    end = words_per_chunk + 1\n",
    "    chunks = []\n",
    "    while start < number_of_words:\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += words_per_chunk\n",
    "        end += words_per_chunk\n",
    "    text = '\\n'.join(chunks)\n",
    "    file = codecs.open(path_doel, 'w', 'utf-8')\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Analyse van de teksten\n",
    "Bij de analyse moet van verschillende stukken in de tekst de overeenkomst worden bepaald. Er zijn hierbij een paar obstakels. Ten eerste worden er in de teksten sommige zinnen aan elkaar geplakt, of er worden juist nieuwe zinnen toegevoegd. Dit zorgt ervoor dat bijvoorbeeld zin 300 in de ene tekst niet dezelfde betekenis heeft als zin 300 in de andere tekst. De overeenkomst tussen deze twee zinnen bepalen zou zinloos zijn. Voor elke zin in de ene tekst moet dus de qua betekenis equivalente zin in de andere tekst gevonden worden. Ten tweede is de overeenkomst tussen twee zinnen bepalen lastig. Hiervoor heb ik verschillende manieren getest.\n",
    "\n",
    "### Bepalen van overeenkomst\n",
    "Bij het bepalen van de overeenkomst tussen twee stukken tekst heb ik verschillende technieken gebruikt: overeenkomst op basis van lettervoorkomen, overeenkomst op basis van woordoverlap en overeenkomst met tf-idf.\n",
    "\n",
    "#### Overeenkomst op basis van lettervoorkomen\n",
    "Bij de overeenkomst op basis van lettervoorkomen wordt gekeken naar het aantal keer dat een letter voorkomt in een stukje tekst. Van beide stukken tekst wordt van elke letter het voorkomen geteld. Vervolgens wordt voor elke letter de absolute waarde van het verschil in voorkomen van die letter in beide teksten berekend. Al deze vershillen worden vervolgens bij elkaar opgeteld. Hoe hoger deze waarde, hoe minder overeenkomst tussen beide zinnen. Langere zinnen hebben echter meer kans op een hogere waarde. Om hiervoor te corrigeren wordt de som van de verschillen gedeeld door de som van het totale aantal letters van beide zinnen. Op deze manier wordt altijd een waarde tussen 0 en 1 verkregen. Het uiteindelijke resultaat is dan 1 minus deze waarde. Op die manier betekent 0 geen overeenkomst en 1 maximale overeenkomst. De functie die ik hiervoor geschreven heb heet 'match_characters()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countcharacter(string, character):\n",
    "    count = 0\n",
    "    for element in string:\n",
    "        if element == character:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "def match_characters(string1, string2):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    verschillen = []\n",
    "    string1 = string1.lower()\n",
    "    string2 = string2.lower()\n",
    "    kenmerkende_letters = ['q', 'w', 'r', 't', 'p', 's', 'd', 'f', 'g', 'h', 'k', 'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm']\n",
    "    for letter in kenmerkende_letters:\n",
    "        list1.append(countcharacter(string1, letter))\n",
    "        list2.append(countcharacter(string2, letter))\n",
    "    list_zipped = zip(list1, list2)\n",
    "    for paar in list_zipped:\n",
    "        waarde1, waarde2 = paar\n",
    "        verschillen.append(abs(waarde1 - waarde2))\n",
    "    for character in string1:\n",
    "        if character not in medeklinkers:\n",
    "            string1 = string1.replace(character, \"\")\n",
    "    for character in string2:\n",
    "        if character not in medeklinkers:\n",
    "            string2 = string2.replace(character, \"\")\n",
    "    match = 1-(sum(verschillen)/(len(string1)+len(string2)))\n",
    "    return match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Overeenkomst op basis van woordoverlap\n",
    "Bij de overeenkomst op basis van woordoverlap wordt gekeken naar hoeveel woorden in beide zinnen voorkomen. Dit aantal wordt vervolgens ook weer gecorrigeerd naar lengte van de zin door het te delen door het gemiddelde aantal woorden per zin. Ook worden woorden die na het verwijderen van niet-kenmerkende letters nog maar uit één letter bestaan, niet meegerekend bij de woordoverlap. De kans is namelijk groot dat deze woorden zonder niet-kenmerkende letters hetzelfde zijn, maar toch verschillend zijn met deze letters erbij. Bovendien zijn dit vaak korte voeg- of lidwoorden. Deze woorden voegen niet veel toe aan de betekenis van een zin. De functie hiervoor heb ik 'match_wordsintersection()' genoemd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_wordsintersection(sentence1, sentence2):\n",
    "    sentence1 = interpunction_uppercase_remove_text(sentence1)\n",
    "    sentence2 = interpunction_uppercase_remove_text(sentence2)\n",
    "    sentence1 = set(sentence1.split())\n",
    "    sentence2 = set(sentence2.split())\n",
    "    intersection = sentence1.intersection(sentence2)\n",
    "    intersection = list(intersection)\n",
    "    for word in intersection:\n",
    "        if len(word) < 2:\n",
    "           intersection.remove(word)\n",
    "    match = (2*len(intersection)/(len(sentence1) + len(sentence2)))\n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Overeenkomst met tf-idf\n",
    "Tf-idf is een techniek om overeenkomst tussen teksten te bepalen die bijvoorbeeld veel gebruikt wordt in zoekmachines om de overeenkomst tussen de tekst op een website en de zoekopdracht te bepalen. De techniek bestaat uit twee delen: Term Frequency (tf) en Inverse Document Frequency (idf).\n",
    "\n",
    "De term frequency is simpelweg het aantal keer dat een bepaalde term voorkomt in een tekst. Vaak wordt dit nog wel gecorrigeerd naar de lengte van de tekst. In dit onderzoek heb ik het aantal keer dat een term in een tekst voorkomt gedeeld door het het aantal keer dat de term die het meeste voorkomt in die tekst, voorkomt. Op die manier is de tf-waarde altijd een getal tussen 0 en 1. \n",
    "\n",
    "De inverse document frequency is er om het belang van bepaalde woorden te bepalen. Wanneer in een tekst bijvoorbeeld heel vaak het woord 'de' voorkomt, kan je nog niets zeggen over de inhoud van die tekst. Juist woorden die normaalgesproken zeldzaam zijn, maar in de tekst wel vaak voorkomen, kunnen veel verraden over de betekenis. De tf-waarde van deze woorden is dus belangrijker. Bij de inverse document frequency wordt daarom in alle te vergelijken tekstfragmenten gezocht naar een term. Het aantal tekstfragmenten waarin de term voorkomt wordt dan geteld. Wanneer een term in meer documenten voorkomt is deze dus minder belangrijk. De idf waarde wordt dan berekend door het totale aantal tekstfragmenten te delen door het aantal tekstfragmenten waarin de term voorkomt en daar de logaritme van te nemen. Vaak wordt bij het aantal tekstfragmenten waarin de term voorkomt nog 1 opgeteld om delen door 0 te voorkomen. Ook in dit onderzoek heb ik dat gedaan.\n",
    "\n",
    "Voor elke term heeft een tekstfragment dus een tf waarde en een idf waarde. Dit heb ik gebruikt door voor een zin van elke term de tf en de idf waarde in de andere zin te bepalen. De idf waarde wordt dan verkregen door te vergelijken met de andere zinnen in de vertaling. Deze tf en idf waarde van elke term heb ik dan met elkaar vermenigvuldigd om de tf-idf-waarde te berekenen. Al deze tf-idf-waardes heb ik bij elkaar opgeteld en gedeeld door het aantal woorden in de zin. Hoe hoger deze waarde dan is, hoe groter de overeenkomst tussen de zinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, string):\n",
    "    terms = string.split()\n",
    "    termsset = set(terms)\n",
    "    terms_enkel = list(termsset)\n",
    "    frequencies = []\n",
    "    for word in terms_enkel:\n",
    "        count = 0\n",
    "        for element in terms:\n",
    "            if element == word:\n",
    "                count += 1\n",
    "        frequencies.append(count)\n",
    "    maximum = max(frequencies)\n",
    "    count = 0\n",
    "    for word in terms:\n",
    "        if word == term:\n",
    "            count += 1\n",
    "    tf = count/maximum\n",
    "    return tf\n",
    "\n",
    "def idf(term, text):\n",
    "    textlist = text.split('\\n')\n",
    "    count = 0\n",
    "    import math\n",
    "    for element in textlist:\n",
    "        words = element.split()\n",
    "        if term in words:\n",
    "            count += 1\n",
    "    idf = math.log(len(textlist)/(count+0.000001))\n",
    "    return idf\n",
    "\n",
    "def tfidf(term, string, text):\n",
    "    tfidf = tf(term, string)*idf(term, text)\n",
    "    return tfidf\n",
    "\n",
    "def match_tfidf(string1, string2, text2):\n",
    "    string1 = interpunction_uppercase_remove_text(string1)\n",
    "    string2 = interpunction_uppercase_remove_text(string2)\n",
    "    text2 = interpunction_uppercase_remove_text(text2)\n",
    "    string1list = string1.split()\n",
    "    match = 0\n",
    "    for element in string1list:\n",
    "        count = 0\n",
    "        waarde = tfidf(element, string2, text2)\n",
    "        for word in chunk1list:\n",
    "            if word == element:\n",
    "                count += 1\n",
    "        match += waarde*count\n",
    "        match = match/len(string1list)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Koppelen van equivalente zinnen\n",
    "Het koppelen van equivalente zinnen is gedaan door van een zin in de ene vertaling de overeenkomst met elke zin in de andere vertaling te bepalen. De zin waarmee de overeenkomst het grootst is, wordt dan gekoppeld aan deze zin. Dit wordt voor elke zin in de vertaling gedaan. Op deze manier onstaat een lijst van gekoppelde zinnen. Hiervoor heb ik drie verschillende functies geschreven op basis van de eerder beschreven drie manieren van bepalen van overeenkomst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_characters(text1, text2):\n",
    "    list1 = text1.split('\\n')\n",
    "    list2 = text2.split('\\n')\n",
    "    paired_strings = []\n",
    "    for string1 in list1:\n",
    "        matchlist = []\n",
    "        for string2 in list2:\n",
    "            matchlist.append(match_characters(string1, string2))\n",
    "        pair = (string1, list2[matchlist.index(max(matchlist))])\n",
    "        paired_strings.append(pair)\n",
    "    return paired_strings\n",
    "\n",
    "def pairs_wordsintersection(text1, text2):\n",
    "    list1 = text1.split('\\n')\n",
    "    list2 = text2.split('\\n')\n",
    "    paired_strings = []\n",
    "    for string1 in list1:\n",
    "        matchlist = []\n",
    "        for string2 in list2:\n",
    "            matchlist.append(match_wordsintersection(string1, string2))\n",
    "        pair = (string1, list2[matchlist.index(max(matchlist))])\n",
    "        paired_strings.append(pair)\n",
    "    return paired_strings\n",
    "\n",
    "def pairs_tfidf(text1, text2):\n",
    "    list1 = text1.split('\\n')\n",
    "    list2 = text2.split('\\n')\n",
    "    paired_strings = []\n",
    "    for string1 in list1:\n",
    "        matchlist = []\n",
    "        for string2 in list2:\n",
    "            matchlist.append(match_tfidf(string1, string2, text2))\n",
    "        pair = (string1, list2[matchlist.index(max(matchlist))])\n",
    "        paired_strings.append(pair)\n",
    "    return paired_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de visualisatie wordt de overeenkomst van een zinnenpaar in een grafiek uitgezet tegen de plaats van dat zinnenpaar. Daarnaast worden de zinnenparen in een document geplakt met de overeenkomst erbij. Aan de hand van dit document kan gecontroleerd worden of zinnen goed aan elkaar gekoppeld zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_characters(path1, path2, path3):\n",
    "    import codecs\n",
    "    import matplotlib\n",
    "    file1 = codecs.open(path1, 'r', 'utf-8')\n",
    "    text1 = file1.read()\n",
    "    file1.close()\n",
    "    file2 = codecs.open(path2, 'r', 'utf-8')\n",
    "    text2 = file2.read\n",
    "    file2.close()\n",
    "    x_place = []\n",
    "    y_match = []\n",
    "    pairs = pairs_characters(text1, text2)\n",
    "    for pair in pairs:\n",
    "        x_place.append(pairs.index(pair)+1)\n",
    "        sting1, string2 = pair\n",
    "        y_match.append(match_characters(string1, string2))\n",
    "    zinnen_paired = \"\"\n",
    "    for element in pairs:\n",
    "        zin1, zin2 = element\n",
    "        zinnen_paired += str(pairs.index(element) + 1) + '\\n\\n' + zin1 + \"\\n\" + zin2 + \"\\n\" + str(y_match[pairs.index(element)]) + '\\n\\n=======================================================\\n'\n",
    "    file3 = codecs.open(path3, 'w', 'utf-8')\n",
    "    file3.write(zinnen_paired)\n",
    "    file3.close()\n",
    "    print(matplotlib.pyplot.plot(x_place, y_match))\n",
    "    \n",
    "    \n",
    "def plot_wordsintersection(path1, path2, path3):\n",
    "    import codecs\n",
    "    import matplotlib\n",
    "    file1 = codecs.open(path1, 'r', 'utf-8')\n",
    "    text1 = file1.read()\n",
    "    file1.close()\n",
    "    file2 = codecs.open(path2, 'r', 'utf-8')\n",
    "    text2 = file2.read\n",
    "    file2.close()\n",
    "    x_place = []\n",
    "    y_match = []\n",
    "    pairs = pairs_wordsintersection(text1, text2)\n",
    "    for pair in pairs:\n",
    "        x_place.append(pairs.index(pair)+1)\n",
    "        string1, string2 = pair\n",
    "        y_match.append(match_wordsintersection(string1, string2))\n",
    "    zinnen_paired = \"\"\n",
    "    for element in pairs:\n",
    "        zin1, zin2 = element\n",
    "        zinnen_paired += str(pairs.index(element) + 1) + '\\n\\n' + zin1 + \"\\n\" + zin2 + \"\\n\" + str(y_match[pairs.index(element)]) + '\\n\\n=======================================================\\n'\n",
    "    file3 = codecs.open(path3, 'w', 'utf-8')\n",
    "    file3.write(zinnen_paired)\n",
    "    file3.close()\n",
    "    print(matplotlib.pyplot.plot(x_place, y_match))\n",
    "    \n",
    "def plot_tfidf(path1, path2, path3):\n",
    "    import codecs\n",
    "    import matplotlib\n",
    "    file1 = codecs.open(path1, 'r', 'utf-8')\n",
    "    text1 = file1.read()\n",
    "    file1.close()\n",
    "    file2 = codecs.open(path2, 'r', 'utf-8')\n",
    "    text2 = file2.read()\n",
    "    file2.close()\n",
    "    x_place = []\n",
    "    y_match = []\n",
    "    pairs = pairs_tfidf(text1, text2)\n",
    "    for pair in pairs:\n",
    "        x_place.append(pairs.index(pair)+1)\n",
    "        string1, string2 = pair\n",
    "        y_match.append(match_tfidf(string1, string2, text2))\n",
    "    zinnen_paired = \"\"\n",
    "    for element in pairs:\n",
    "        zin1, zin2 = element\n",
    "        zinnen_paired += str(pairs.index(element) + 1) + '\\n\\n' + zin1 + \"\\n\\n\" + zin2 + \"\\n\\n\" + str(y_match[pairs.index(element)]) + '\\n\\n=======================================================\\n'\n",
    "    file3 = codecs.open(path3, 'w', 'utf-8')\n",
    "    file3.write(zinnen_paired)\n",
    "    file3.close()\n",
    "    print(matplotlib.pyplot.plot(x_place, y_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bepalen van overeenkomst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij het bepalen van de overeenkomst wordt steeds gekeken naar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
